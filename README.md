## High Level Architecture

The high level design for dynamic credential generation is depicted in the diagram below:
![Architecture Overview](assets/architecture.svg)
1. [Domsed](https://github.com/cerebrotech/domsed) is used to inject a side-car in the Workspace Pod.
2. The side-car is a Flask App which generates AWS Credentials for each role the user is mapped to and loads these in a shared folder (with the `run` container)   
3. The injected side-car communicates with Hashicorp Vault instance using a token mounted as a secret. 
4. Vault communicates with AWS using a AWS service account with limited privileges to generate credentials using STS but limited by permission boundaries

In document below we will demonstrate the following:

1. Install Vault locally and run the side-car as a stand-alone Flask App
2. Demonstrate the `run` container functionality by generating credentials via the side-car
3. Extending the design to a central version of Vault used by a Domino Installation

## Use-Case
The use-case this solution addresses is as follows:

1. Domino has 3 users namely, `test-user-1`,`test-user-2` and  `test-user-3`
2. The customer is named `domino-demo-customer`
3. In the customer AWS account is a bucket creatively named as `domino-demo-customer-bucket`
4. The bucket has the following key/value pairs
   - `test-user-1/whoami.txt` = `I am test-user-1`
   - `test-user-2/whoami.txt` = `I am test-user-2`
   - `test-user-3/whoami.txt` = `I am test-user-3`
5. All users have access to their own sub-folders
6. Users can be provided access to other users sub-folders

## Local Installation and Testing

In this section we will prepare the foundation for adding Credential Propagation to Domino Apps
The overall Vault [Getting  Started tutorial](https://learn.hashicorp.com/collections/vault/getting-started) is a good place to get started with understanding how to use Vault.

### Installation of Vault 
1. First [install](https://learn.hashicorp.com/tutorials/vault/getting-started-install?in=vault/getting-started) Vault locally
2. [Start](https://learn.hashicorp.com/tutorials/vault/getting-started-dev-server?in=vault/getting-started) Vault in Dev mode.
   The command to run on your localhost terminal is `vault server -dev`
   In the output note the following. You will need all three (Examples shown below)   
   - `export VAULT_ADDR=http://127.0.0.1:8200`
   - `Unseal Key: G+5edKxySWwPydj8BB3mUBiJxUamo1pKequX5DS26kY=`
   - `Unseal Key: G+5edKxySWwPydj8BB3mUBiJxUamo1pKequX5DS26kY=`
     `Root Token:  s.mJTaOPfOLMyECRnilupJNSPb`
     
   Ensure that you protect the root token and the unseal key. Write to your AWS Secrets Engine in AWS
   
3. Go to your browser and login to `http://127.0.0.1:8200` using the root token.
4. Configure the following environment variables
   ```shell
   export VAULT_ADDR=http://127.0.0.1:8200
   export VAULT_TOKEN=s.mJTaOPfOLMyECRnilupJNSPb #Your token will be different
   # export VAULT_NS=domino/my_install #Namespaces are an enterprise feature.You don't need this in Open Source Version
   ```

## Quick Install

For local testing add the following environment variables and run `./install_local.sh`
```shell
export VAULT_ADDR=http://127.0.0.1:8200 #Replace with your own url for remote vault
export VAULT_TOKEN="ADD HERE"
export docker_version=beta_v3 #Add the side-car to a docker registry with a tag
./scripts/install_local.sh $docker_version
```

For remote vault run `./install.sh`
```shell
export VAULT_ADDR="ADD HERE"
export VAULT_TOKEN="ADD HERE"
export VAULT_NAMESPACE="ADD HERE"
export docker_version=beta_v3 #Add the side-car to a docker registry with a tag
./scripts/install.sh $docker_version
```

## What happens in the quick install
Internally the `./scripts/install.sh $docker_version` performs the following steps

### Create the bucket

We will create this bucket and its contents by invoking the following program

```
    python ./src/admin/create_demo_customer_bucket.py <CONFIG_FILE_NAME> <AD_MAPPING_FILE>
    #Ex. python ./src/admin/create_demo_customer_bucket.py ./config/install_config.json ./config/user_names.json
    #Invoking the below has the same effect because those paths are defaults
    python ./src/admin/create_demo_customer_bucket.py 
```
The `./config/install_config.json` contains the aws iam user vault will use as well as the demo-bucket name
```json
{
  "domino_vault_user" : "vault-domino",
  "customer_s3_bucket" : "domino-test-customer-bucket"
}
```

Think of the file `./config/users.json` to be auto-generated by listing the AD roles mapped to AWS roles. It
is a mapping of users to each AWS role they are mapped through via AWS SAML Integration with LDAP/AD

### Create the IAM User to be used by Vault
Next create an IAM user whose permissions vault will assume to make STS calls

```
python ./src/admin/configure_vault_aws_user.py <domino_vault_user> <customer_s3_bucket>
```
Default values are:
```
domino_vault_user = vault-domino
customer_s3_bucket = domino-demo-customer-bucket
```
This call will place a file `$PROJECT_ROOT/aws_creds/creds.json` which will contain the 
aws credentials of the new user. This allows the above command to be invoked without the 
third parameter as long as the file is present as  `./aws_creds/creds.json`

This user will assume the following permissions:
1. `./aws_policy_templates/VAULT_USER_POLICY_TEMPLATE.json` with name `VAULT_DOMINO_USER`
2. `./aws_policy_templates/PERMISSIONS_BOUNDARY_POLICY_TEMPLATE.json` with name `CUSTOMER_BOUNDARY_PERMISSION`

The latter policy allows the `vault-domino` user full permissions to the bucket `domino-demo-customer-bucket`

The access keys are placed in the file `./aws_creds/creds.json`

### Create roles in AWS to enable write access to each sub-folder of the bucket

To create a list of demo roles and policies review the file `./config/users.json`
```json
{
  "AD_GROUPS": ["GRP-1","GRP-2","GRP-3"],
  "AWS_ROLES":  ["sample_domino_customer_role_1","sample_domino_customer_role_2","sample_domino_customer_role_3"],
  "AWS_ROLES_TO_POLICIES_MAPPING": {
    "sample_domino_customer_role_1": ["vault_test-user-1_policy","vault_test-user-2_policy"],
    "sample_domino_customer_role_2": ["vault_test-user-2_policy"],
    "sample_domino_customer_role_3": ["vault_test-user-3_policy"]
  },
  "AD_GROUP_TO_AWS_ROLE_MAPPING": {"GRP-1": "sample_domino_customer_role_1",
                                   "GRP-2": "sample_domino_customer_role_2",
                                   "GRP-3": "sample_domino_customer_role_3"},
  "AD_GROUP_TO_USER_MAPPING": {"GRP-1": ["test-user-1","test-user-2"],
                               "GRP-2": ["test-user-2"],
                               "GRP-3": ["test-user-3"]}
}
```
We have the following relationship between AD Group/Roles and Users in the above JSON file

| AD Group      | AWS Role       | AD Members
|--------------|-----------------|--------------
|GRP-1   |sample_domino_customer_role_1  |test-user-1
|    | |test-user-2
GRP-1  |sample_domino_customer_role_2|test-user-2
GRP-1   |sample_domino_customer_role_3|test-user-3

To create the necessary policies and roles mapped to user (SSO Emulation) run the following command
```shell
python ./src/admin/configure_ldap_based_aws_policies_and_roles.py.py
```


- Customer has Active Directory (AD) groups `AD_GROUPS`. User's in AD are mapped to an AD group based on `AD_GROUP_TO_USER_MAPPING`
- Imagine the customer has mapped two AD Groups (`AD_GROUPS`) to two AWS Roles (`AWS_ROLES`)
- AD Group is mapped to AWS Role in a one to one mapping based on `AD_GROUP_TO_AWS_ROLE_MAPPING`
- Each of those roles is mapped to one or more policies based on `AWS_ROLES_TO_POLICIES_MAPPING`

This file can be auto-generated based on querying Active Directory. All except `AWS_ROLES_TO_POLICIES_MAPPING` are needed to automate 
the process. `AWS_ROLES_TO_POLICIES_MAPPING` is something that happens outside of this solution as part of Role Governance.


### Configure Vault for Domino App's Cred Prop

Note that the following env variables are configured:
`VAULT_ADDR`
`VAULT_TOKEN`
 

Copy the Access and Secret keys and configure these env variables and run the command `./scripts/configure-vault.sh`
```shell
export AWS_ACCESS_KEY_ID=$(cat ./aws_creds/creds.json | jq -r '.AccessKeyId')
export AWS_SECRET_ACCESS_KEY=$(cat ./aws_creds/creds.json | jq -r '.SecretAccessKey')
./scripts/configure-vault.sh
```
This achieves the following:

1. Generate an APP TOKEN which allows Domino to communicate with Vault. You need the `VAULT_ADDR` and `VAULT_TOKEN` variables set at this point
   We will use this as the vault token from this point on to communicate
   ```shell
   
   # First define the policy (access permissions) for the token we are about to generate
   # This defines the permission boundaries for the vault token
   
   vault policy write domino ./vault_policies/domino.hcl
   
   #Make the token valid for 25 hours and renew every 24 hours. This is necessary
   #because a token can be renewable for upto 768 hours but always expires in 24 hours
   
   export VAULT_TOKEN=$(vault token create -orphan -policy=domino -period=25h  -ttl 25h -format=json | jq .auth.client_token | sed 's/"//g' )
  
   #Write to a file (Or your AWS Secrets Engine) Securing is important. Token gives access to vault
   echo $VAULT_TOKEN > ./root/etc/vault/token
   ```
2. Enable the AWS Secrets Engine in Vault
   ```shell
   #There are many more (https://www.vaultproject.io/docs/secrets)
   vault secrets enable aws
   ```
3. Vault generates AWS credentials by requesting AWS STS for these credentials. The user `vault-domino` was 
   created with the appropriate permissions to generate "AssumeRole", "GetFederationToken" privileges.
   We now configure Vault with the keys of this user to perform the actions on Domino's behalf.
   ```shell
   vault write aws/config/root \
       access_key=$AWS_ACCESS_KEY_ID \
       secret_key=$AWS_SECRET_ACCESS_KEY \
       region=us-west-2
   ```
4. Immediately rotate them. This ensures that now only Vault knows the SecretAccessKey
   ```shell
      #Immediately rotate them. Now only vault knows the secret key. The existing ones are replaced
      vault write -f aws/config/rotate-root
   ```

### Configure the k8s cluster
First install Domsed and execute the command (Not included in the `install_local.sh`)
```shell
./scripts/install-vault-cred-prop-to-domino.sh $docker_version
```
This performs the following steps:

1. Write token to a k8s secret (Step only makes sense when Vault is not deployed in local mode)
   ```shell
      export compute_namespace=domino-compute
      kubectl delete secret vault-token -n $compute_namespace
      kubectl create secret generic vault-token --from-literal=token=$VAULT_TOKEN -n $compute_namespace
   ```
2. Create a k8s config-map to provide connection details to Vault (Step only makes sense when Vault is not deployed in local mode)
   ```
      export compute_namespace=domino-compute
      export DOMSED_VAULT_CONFIGMAP_NAME=dynamic-aws-creds-config
      kubectl delete configmap ${DOMSED_VAULT_CONFIGMAP_NAME} -n $compute_namespace
      cat <<EOF | kubectl create -n ${compute_namespace} -f -
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ${DOMSED_VAULT_CONFIGMAP_NAME}
      data:
        dynamic-aws-creds-config: |-
          {
            "vault_endpoint": "${VAULT_ADDR}",
            "polling_interval_in_seconds" : 300,
            "refresh_threshold_in_seconds" : 600,
            "vault_namespace" : "${VAULT_NAMESPACE}",
            "lease_increment" : 3600,
            "default_user" : "default"
          }
      EOF
   ```
2. Deploy the Domsed Mutation to inject the Vault Side Car (Do not do this on the local machine install)
   ```shell
   #Deploy the DOMSED Mutation to perform AWS Credential Propagation using Dynamic Secrets
   kubectl delete -f ./mutations/dynamic-vault-based-creds-mutation.yaml -n $platform_namespace
   kubectl create -f ./mutations/dynamic-vault-based-creds-mutation.yaml -n $platform_namespace
   ```
   
    The mutation is described as follows:
   ```json
   kind: Mutation
   metadata:
     name: dynamic-vault-based-creds-mutation
   rules:
     -
       labelSelectors:
         - "dominodatalab.com/workload-type in (Workspace,Batch,Scheduled)"
       insertVolumeMounts:
         containerSelector:
           - run
         volumeMounts:
           - name: log-volume
             mountPath: /etc/log/vault/
             readOnly: true
           - mountPath: /etc/.aws/
             name: dynamic-aws-creds
             readOnly: true
   
       insertContainer:
         containerType: app
         spec:
           name: z-vault-client-side-car
           image: quay.io/domino/vault-creds-prop:latest
           args: [ '/',5010 ]
           volumeMounts:
             - name: log-volume
               mountPath: /etc/log/vault/
             - name: podinfo
               mountPath: /etc/labels/
             - name: dynamic-aws-creds-config-volume
               mountPath: /etc/config/
             - mountPath: /etc/.aws/
               name: dynamic-aws-creds
             - mountPath: /etc/vault/
               name: vault-token
               readOnly: true
       insertVolumes:
         - emptyDir: { }
           name: dynamic-aws-creds
         - emptyDir: { }
           name: log-volume
         - name: dynamic-aws-creds-config-volume
           configMap:
             name: dynamic-aws-creds-config
         - name: vault-token
           secret:
             secretName: vault-token
         - name: podinfo
           downwardAPI:
             defaultMode: 420
             items:
               - fieldRef:
                   fieldPath: metadata.labels
                 path: "labels"
       modifyEnv:
         containerSelector:
         - run
         env:
         - name: AWS_SHARED_CREDENTIALS_FILE
           value: "/etc/.aws/credentials"
   
   ```

The mutation produces the following:

1. When the workspace is launched a side-car gets attached with the container name `z-vault-client-side-car`
2. The side car runs as a Flask endpoint on port `5010`
3. The following volumes are mounted in the side-car
   - `/etc/log/vault/` to store the logs produced by the side-car.
   - `/etc/labels/` all pod labels are stored here in a file called `labels`. It is used to identify the `domino-user-name` to which the workspace belongs
   - `/etc/config/` This folder contains the file `dynamic-aws-creds-config` which is to be loaded from the config map `dynamic-aws-creds-config`
   - `/etc/vault/` this folder contains a file `token` which is the vault token used by the side-car to communicate with vault (Protect this token). It is loaded from the secret `vault-token`
   - `/etc/.aws/` this folder is used by the side-car to write the `credentials` file for the user
4. The folder `/etc/.aws` is shared with the run-container (READONLY)
5. An env variable is injected into the `run` container by the name `AWS_SHARED_CREDENTIALS_FILE` and set to `/etc/.aws/credentials`. This sets the default location of the aws credentials file.
   

### Map roles to users in Vault

The script `src/admin/configure_vault_aws_roles.py` configures 
1. The mapping between the user and roles in the `kv` secret store (http://127.0.0.1:8200/ui/vault/secrets/kv/list/domino/user/)
2. The type of credentials for each role in the `aws` secret store (http://127.0.0.1:8200/ui/vault/secrets/aws/list)
   - We make use of the token known as `Federation Token`
   - Each role is attached policies
     ```json
        {"credential_type": "federation_token", "policy_arns": [#Comma seperated list of AWS Policy ARNS]}
     ```
   - Federation token has the benefit that it cannot assume any roles even if the policy attached to it permits it.
3. You can generate a sample set of credentials as follows
```shell
curl --location --request GET 'http://127.0.0.1:8200/v1/aws/creds/vault-sample_domino_customer_role_3' \
--header 'X-Vault-Token: YOUR_TOKEN' \
--header 'Content-Type: application/json' \
--data-raw ' {"credential_type": "federation_token", "policy_arns": []}'
```
This results in 
```json
{
    "request_id": "816e9df0-0658-c5e9-164c-f731dae4a2e6",
    "lease_id": "aws/creds/vault-sample_domino_customer_role_3/FiDE5kjqfzHQB9H0IuuHcDVG",
    "renewable": false,
    "lease_duration": 900,
    "data": {
        "access_key": "ASIA5XXXXX",
        "secret_key": "V5FXXXX",
        "security_token": "Fwoxxx"
    },
    "wrap_info": null,
    "warnings": null,
    "auth": null
}
```



## Test the Side-Car Locally

### Start the Side-Car Locally


The root folder emulates the `/` folder in the actual side-car. Add the following files to it 
1. `./root/etc/config/dynamic-aws-creds-config`
```json
{
  "vault_endpoint":"http://127.0.0.1:8200",
  "vault_namespace":"",
  "polling_interval_in_seconds" : 300,
  "refresh_threshold_in_seconds" : 600,
  "lease_increment" : 300,
  "default_user" : "default"
}
```

2. `./root/var/log/vault/` - This is where the side-car writes the `app.log`

3. `./root/etc/vault/token` - Add your `vault-token` here. It should be already added by the `install-local.sh` script

4. `./root/etc/.aws` - The `credentials` file will be created in this folder

5. `./root/etc/labels/labels` - This file will contain the pod labels. For now only add the followint entry to it
```
dominodatalab.com/starting-user-username=test-user-2
```



To start the side-car locally run the following command in the project root folder
```shell
  python  python3 src/mutation/vault_creds_generator.py ./root 5010
```
In another shell run this command
```shell
mkdir ${PWD}/root/etc/.aws/
export AWS_SHARED_CREDENTIALS_FILE=${PWD}/root/etc/.aws/credentials 

cat $AWS_SHARED_CREDENTIALS_FILE 
#You will see a profile per role similar to domino creds prop. 

aws sts get-caller-identity --profile=vault-sample_domino_customer_role_2
#Notice the output. We are using federation tokens here

#This should succeed
aws --profile=vault-sample_domino_customer_role_2 s3 cp s3://domino-test-customer-bucket/test-user-2/whoami.txt  /tmp/

#This will fail with access forbidden error
aws --profile=vault-sample_domino_customer_role_2 s3 cp s3://domino-test-customer-bucket/test-user-3/whoami.txt  /tmp/


```
You can change to any of the three users(`test-user-1`, `test-user-2` and `test-user-3`) here and restart the program to emulate each users side-car


## Remote Installation and Local Testing
```
export VAULT_ADDR=http://127.0.0.1:8200 #Replace with your own url for remote vault
export VAULT_TOKEN="ADD HERE"
export VAULT_NS="" #Add your own namespace. Only valid for enterprise version. If so replace with export VAULT_NS="dominoaws" o
export docker_version=beta_v3 #Add the side-car to a docker registry with a tag
./scripts/install.sh $docker_version

python src/mutation/app-test.py ./root
python src/mutation/app_vault_sidecar.py ./root 5010
```
Follow the same steps for testing. The `app_vault_sidecar.py` connects to remote vault now via `app-test.py`

## Summary

### Benefits of this approach

1. Portable - It works for all clouds and even Active Directory based authentication because Vault supports [multiple](https://www.vaultproject.io/docs/secrets) secrets engines
2. Simplified User Interface - The endpoint remains the same from the App perspective. The side-car does the heavy lifting
3. Keys are short-lived and access is bounded (Tested for AWS using Federation User Token)

### Drawbacks of this approach

1. Needs Vault to be installed either in K8s. Vault as a service is available. It can also be deployed on AWS and managed by the customer.

### Security Risks

This approach needs an intensive code-review. The writer of the app can emulate any user because the user-name
and project-name is passed as plain text in headers and used directly